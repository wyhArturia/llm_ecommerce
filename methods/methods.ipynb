{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: WANDS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GPT prediction results\n",
    "list_id_gpt_instruct_train, list_truth_gpt_instruct_train_bool, list_pred_gpt_instruct_train_bool, _ = pickle.load(open(\"../data/relevance_gpt_rslt/rslt_gpt_instruct_20240424_train.p\", \"rb\"))\n",
    "list_id_gpt_instruct_test, list_truth_gpt_instruct_test_bool, list_pred_gpt_instruct_test_bool, _ = pickle.load(open(\"../data/relevance_gpt_rslt/rslt_gpt_instruct_20240424_test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use GPT to predict the first 20000 samples in the training set\n",
    "list_id_llm_train_20k = list_id_gpt_instruct_train[:20000]\n",
    "\n",
    "# load all the data for machine learning model\n",
    "dict_data_all, dict_id = pickle.load(open(\"../data/relevance_dataset/dict_data_20240424.p\", \"rb\"))\n",
    "list_id_train = dict_id['project_1']['list_id_train']\n",
    "list_id_test = dict_id['project_1']['list_id_test']\n",
    "dict_id2info = dict_data_all['dict_id2info']\n",
    "\n",
    "# split the dataset, x_train_llm_20k are those with llm prediction\n",
    "x_train = np.array([dict_id2info[index]['embedding_concat'] for index in list_id_train])\n",
    "x_test = np.array([dict_id2info[index]['embedding_concat'] for index in list_id_test])\n",
    "x_train_llm_20k = np.array([dict_id2info[index]['embedding_concat'] for index in list_id_llm_train_20k])\n",
    "\n",
    "y_train = np.array([dict_id2info[index]['label_truth_bool'] for index in list_id_train])\n",
    "y_train_llm_20k = np.array([dict_id2info[index]['label_truth_bool'] for index in list_id_llm_train_20k])\n",
    "y_test = np.array([dict_id2info[index]['label_truth_bool'] for index in list_id_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hasee\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the ml model by using the logistic regression for embedded word vectors\n",
    "model_ml = LogisticRegression()\n",
    "model_ml.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the scores and binary labels on the train/test set with the ml model\n",
    "y_train_llm_20k_score = model_ml.predict_proba(x_train_llm_20k)[:, 1]\n",
    "y_train_llm_20k_label = model_ml.predict(x_train_llm_20k)\n",
    "\n",
    "y_train_ml_score = model_ml.predict_proba(x_train)[:, 1]\n",
    "y_test_ml_score = model_ml.predict_proba(x_test)[:, 1]\n",
    "y_train_ml_label = model_ml.predict(x_train)\n",
    "y_test_ml_label = model_ml.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear combination of ml and llm estimators in section 3\n",
    "\n",
    "class model_linear_combine:\n",
    "    def __init__(self, num_of_piece=1):\n",
    "        self.n = num_of_piece\n",
    "        self.alpha = np.zeros((num_of_piece))\n",
    "\n",
    "    def fit(self, y_ml, y_llm, y_true):\n",
    "        # fit piecewise linear alpha for alpha * model_ml + (1-alpha) * model_llm through OLS to minimize the MSE\n",
    "        # y_ml: prediction of model_ml; y_llm: prediction of model_llm; y_true: true label.\n",
    "        # y_ml and y_llm should be in the same scale.\n",
    "        # y_true should be binary.\n",
    "        # alpha should be in [0, 1]\n",
    "\n",
    "        # assign data to group i if y_ml in [i/n, (i+1)/n), here n is the number of pieces\n",
    "        self.group = np.floor(y_ml * self.n).astype(int)\n",
    "\n",
    "        # fit alpha, for each group i, use all data in group i to fit alpha_i to minimize the MSE through OLS\n",
    "        for i in range(self.n):\n",
    "            mask = self.group == i\n",
    "            if np.sum(mask) > 0:    \n",
    "                tmp = np.sum((y_ml[mask] - y_llm[mask])*(y_true[mask] - y_llm[mask])) / np.sum((y_ml[mask] - y_llm[mask])**2)\n",
    "                self.alpha[i] = tmp\n",
    "        \n",
    "    def predict(self, y_ml, y_llm):\n",
    "        # predict the label based on the fitted alpha\n",
    "        group = np.floor(y_ml * self.n).astype(int)\n",
    "        y_pred = y_ml * self.alpha[group] + y_llm * (1 - self.alpha[group])\n",
    "\n",
    "        # return the prediction score\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration method in section 4\n",
    "class model_calibration:\n",
    "    def __init__(self, m):\n",
    "        self.m = m \n",
    "        \n",
    "        # delta (m*2) are \\hat{\\Delta} in the paper; here we use delta_{i,l} for if the ml prediction is in [i/m, (i+1)/m) and llm prediction is l for simplicity,\n",
    "        # which is slightly different from that in the paper.\n",
    "        self.delta = np.zeros((m,2)) \n",
    "\n",
    "    def fit(self, y_ml, y_llm, y_true):\n",
    "        # fit delta for each group i, l through the sample mean\n",
    "        for i in range(self.m):\n",
    "            for l in range(2):\n",
    "                mask = (np.floor(y_ml * self.m) == i) & (y_llm == l)\n",
    "                if np.sum(mask) > 0:\n",
    "                    tmp = np.mean(y_true[mask]-y_ml[mask])\n",
    "                    self.delta[i,l] = tmp\n",
    "    \n",
    "    def predict(self, y_ml, y_llm):\n",
    "        # predict the label based on the fitted delta\n",
    "        y_pred = np.zeros_like(y_ml)\n",
    "        for i in range(self.m):\n",
    "            for l in range(2):\n",
    "                mask = (np.floor(y_ml * self.m) == i) & (y_llm == l)\n",
    "                y_pred[mask] = y_ml[mask] + self.delta[i,l]\n",
    "\n",
    "        # return the prediction score\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_acc(model, model_name_string, y_train_ml, y_train_llm, y_train_true, y_test_ml, y_test_llm, y_test_true):\n",
    "    model.fit(y_train_ml, y_train_llm, y_train_true)\n",
    "\n",
    "    y_train_pred_score = model.predict(y_train_ml, y_train_llm)\n",
    "    y_train_pred_label = (y_train_pred_score>0.5).astype(int)\n",
    "\n",
    "    y_test_pred_score = model.predict(y_test_ml, y_test_llm)\n",
    "    y_test_pred_label = (y_test_pred_score>0.5).astype(int)\n",
    "\n",
    "    acc_train = np.mean(y_train_pred_label==y_train_true)\n",
    "    acc_test = np.mean(y_test_pred_label==y_test_true)\n",
    "    print('method: '+model_name_string+' \\n train acc/test acc = ',(acc_train,acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: ml \n",
      " train acc/test acc =  (0.8271, 0.8027333333333333)\n",
      "method: llm \n",
      " train acc/test acc =  (0.7407, 0.7750666666666667)\n"
     ]
    }
   ],
   "source": [
    "acc_train_ml = np.mean(y_train_llm_20k_label==y_train_llm_20k)\n",
    "acc_test_ml = np.mean(y_test_ml_label==y_test)\n",
    "acc_train_llm = np.mean(y_train_llm_20k==list_pred_gpt_instruct_train_bool)\n",
    "acc_test_llm = np.mean(y_test==list_pred_gpt_instruct_test_bool)\n",
    "print('method: ml \\n train acc/test acc = ',(acc_train_ml,acc_test_ml))\n",
    "print('method: llm \\n train acc/test acc = ',(acc_train_llm,acc_test_llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: linear combine with 1 piece \n",
      " train acc/test acc =  (0.84575, 0.8383333333333334)\n"
     ]
    }
   ],
   "source": [
    "# naive method with 1 piece to fit linear weight, see section 3.1 in the paper\n",
    "num_of_piece = 1 \n",
    "model_linear_naive = model_linear_combine(num_of_piece=num_of_piece)\n",
    "\n",
    "train_test_acc(model_linear_naive, 'linear combine with '+str(num_of_piece)+' piece', y_train_llm_20k_score, np.array(list_pred_gpt_instruct_train_bool), y_train_llm_20k,\n",
    "y_test_ml_score, np.array(list_pred_gpt_instruct_test_bool), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: linear combine with 4 piece \n",
      " train acc/test acc =  (0.8451, 0.8461333333333333)\n"
     ]
    }
   ],
   "source": [
    "# naive method with several pieces to fit linear weight, see section 3.2 in the paper\n",
    "\n",
    "# tune the num of pieces here as you want; we use equal size pieces\n",
    "num_of_piece = 4\n",
    "model_linear_piecewise = model_linear_combine(num_of_piece=num_of_piece)\n",
    "\n",
    "train_test_acc(model_linear_piecewise, 'linear combine with '+str(num_of_piece)+' piece', y_train_llm_20k_score, np.array(list_pred_gpt_instruct_train_bool), y_train_llm_20k,\n",
    "y_test_ml_score, np.array(list_pred_gpt_instruct_test_bool), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: calibration with 10 bins \n",
      " train acc/test acc =  (0.8483, 0.8404666666666667)\n"
     ]
    }
   ],
   "source": [
    "# calibration method, see section 4 in the paper\n",
    "\n",
    "# tune the number of bins for calibration\n",
    "calibration_m = 10\n",
    "model_calibrate = model_calibration(calibration_m)\n",
    "\n",
    "train_test_acc(model_calibrate, 'calibration with '+str(calibration_m)+' bins', y_train_llm_20k_score, np.array(list_pred_gpt_instruct_train_bool), y_train_llm_20k,\n",
    "y_test_ml_score, np.array(list_pred_gpt_instruct_test_bool), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Other Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for other datasets\n",
    "\n",
    "def validate_data(path_gpt_train, path_gpt_test, path_data, **kargs):\n",
    "    list_id_gpt_train, list_truth_bool_train, list_gpt_pred_bool_train, list_rslt_detail_train = pickle.load(open(path_gpt_train, \"rb\"))\n",
    "    list_id_gpt_test, list_truth_bool_test, list_gpt_pred_bool_test, list_rslt_detail_test = pickle.load(open(path_gpt_test, \"rb\"))\n",
    "    dict_data_all, dict_id2info, dict_id = pickle.load(open(path_data, \"rb\"))\n",
    "\n",
    "\n",
    "    list_id_train = dict_id['list_id_train']\n",
    "    list_id_test = dict_id['list_id_test']\n",
    "\n",
    "    x_train = np.array([dict_id2info[index]['embedding'] for index in list_id_train])\n",
    "    x_test = np.array([dict_id2info[index]['embedding'] for index in list_id_test])\n",
    "\n",
    "    y_train = np.array([dict_id2info[index]['label_bool'] for index in list_id_train])\n",
    "    y_test = np.array([dict_id2info[index]['label_bool'] for index in list_id_test])\n",
    "\n",
    "    # ml model\n",
    "    model_ml = LogisticRegression()\n",
    "    model_ml.fit(x_train, y_train)\n",
    "    #print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "    y_train_ml_score = model_ml.predict_proba(x_train)[:, 1]\n",
    "    y_test_ml_score = model_ml.predict_proba(x_test)[:, 1]\n",
    "    y_train_ml_label = model_ml.predict(x_train)\n",
    "    y_test_ml_label = model_ml.predict(x_test)\n",
    "\n",
    "    acc_train_ml = np.mean(y_train_ml_label==y_train)\n",
    "    acc_test_ml = np.mean(y_test_ml_label==y_test)\n",
    "    acc_train_llm = np.mean(y_train==list_gpt_pred_bool_train)\n",
    "    acc_test_llm = np.mean(y_test==list_gpt_pred_bool_test)\n",
    "    print('method: ml \\n train acc/test acc = ',(acc_train_ml,acc_test_ml))\n",
    "    print('method: llm \\n train acc/test acc = ',(acc_train_llm,acc_test_llm))\n",
    "\n",
    "    # linear combination with 1 piece\n",
    "    num_of_piece = 1 \n",
    "    model_linear_naive = model_linear_combine(num_of_piece=num_of_piece)\n",
    "    train_test_acc(model_linear_naive, 'linear combine with '+str(num_of_piece)+' piece', y_train_ml_score, np.array(list_gpt_pred_bool_train), y_train,\n",
    "    y_test_ml_score, np.array(list_gpt_pred_bool_test), y_test)\n",
    "\n",
    "    # linear combination with several pieces\n",
    "    num_of_piece = 4 if 'num_of_piece' not in kargs else kargs['num_of_piece']\n",
    "    model_linear_piecewise = model_linear_combine(num_of_piece=num_of_piece)\n",
    "    train_test_acc(model_linear_piecewise, 'linear combine with '+str(num_of_piece)+' piece', y_train_ml_score, np.array(list_gpt_pred_bool_train), y_train,\n",
    "    y_test_ml_score, np.array(list_gpt_pred_bool_test), y_test)\n",
    "\n",
    "    # calibration method\n",
    "    calibration_m = 20 if 'calibration_m' not in kargs else kargs['calibration_m']\n",
    "    model_calibrate = model_calibration(calibration_m)\n",
    "\n",
    "    train_test_acc(model_calibrate, 'calibration with '+str(calibration_m)+' bins', y_train_ml_score, np.array(list_gpt_pred_bool_train), y_train,\n",
    "    y_test_ml_score, np.array(list_gpt_pred_bool_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validating Yelp Data\n",
      "method: ml \n",
      " train acc/test acc =  (0.7344285714285714, 0.691)\n",
      "method: llm \n",
      " train acc/test acc =  (0.7295714285714285, 0.724)\n",
      "method: linear combine with 1 piece \n",
      " train acc/test acc =  (0.7608571428571429, 0.739)\n",
      "method: linear combine with 4 piece \n",
      " train acc/test acc =  (0.7674285714285715, 0.742)\n",
      "method: calibration with 20 bins \n",
      " train acc/test acc =  (0.7711428571428571, 0.743)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Yelp dataset \n",
    "\n",
    "path_gpt_train = \"../data/other_gpt_rslt/rslt_gpt_20240429_yelp_train.p\"\n",
    "path_gpt_test = \"../data/other_gpt_rslt/rslt_gpt_20240429_yelp_test.p\"\n",
    "path_data = \"../data/other_dataset/yelp/data/dict_data_20240429_yelp.p\"\n",
    "\n",
    "print(\"\\n\\nValidating Yelp Data\")\n",
    "validate_data(path_gpt_train, path_gpt_test, path_data, num_of_piece = 4, calibration_m = 20) # tune the parameters here as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validating Emotion Data\n",
      "method: ml \n",
      " train acc/test acc =  (0.8669907202828104, 0.7993827160493827)\n",
      "method: llm \n",
      " train acc/test acc =  (0.7445868316394167, 0.7592592592592593)\n",
      "method: linear combine with 1 piece \n",
      " train acc/test acc =  (0.8727353071144498, 0.8055555555555556)\n",
      "method: linear combine with 10 piece \n",
      " train acc/test acc =  (0.8740609809986744, 0.8117283950617284)\n",
      "method: calibration with 10 bins \n",
      " train acc/test acc =  (0.8718515245249668, 0.8055555555555556)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Emotion dataset\n",
    "\n",
    "path_gpt_train = \"../data/other_gpt_rslt/rslt_gpt_20240429_emotion_train.p\"\n",
    "path_gpt_test = \"../data/other_gpt_rslt/rslt_gpt_20240429_emotion_test.p\"\n",
    "path_data = \"../data/other_dataset/emotion/data/dict_data_20240429_emotion.p\"\n",
    "\n",
    "print(\"\\n\\nValidating Emotion Data\")\n",
    "validate_data(path_gpt_train, path_gpt_test, path_data, num_of_piece = 10, calibration_m = 10) # tune the parameters here as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Validating Hate Data\n",
      "method: ml \n",
      " train acc/test acc =  (0.7384228790235731, 0.7169179229480737)\n",
      "method: llm \n",
      " train acc/test acc =  (0.6544214431015914, 0.6716917922948074)\n",
      "method: linear combine with 1 piece \n",
      " train acc/test acc =  (0.7371066172071318, 0.7202680067001676)\n",
      "method: linear combine with 4 piece \n",
      " train acc/test acc =  (0.7387818595189661, 0.7236180904522613)\n",
      "method: calibration with 12 bins \n",
      " train acc/test acc =  (0.7414143831518487, 0.7311557788944724)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Hate dataset\n",
    "\n",
    "path_gpt_train = \"../data/other_gpt_rslt/rslt_gpt_20240429_hate_train.p\"\n",
    "path_gpt_test = \"../data/other_gpt_rslt/rslt_gpt_20240429_hate_test.p\"\n",
    "path_data = \"../data/other_dataset/hate/data/dict_data_20240429_hate.p\"\n",
    "\n",
    "print(\"\\n\\nValidating Hate Data\")\n",
    "validate_data(path_gpt_train, path_gpt_test, path_data, num_of_piece = 4, calibration_m = 12) # tune the parameters here as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Transfer learning with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for tranfer learning part, see section 5 in the paper\n",
    "\n",
    "list_id_gpt_instruct_bed_test, list_truth_gpt_instruct_bed_test_bool, list_pred_gpt_instruct_bed_bool, _ = pickle.load(open(\"../data/relevance_gpt_rslt/rslt_gpt_instruct_20240424_project2_bed_test.p\", \"rb\"))\n",
    "dict_data_all, dict_id = pickle.load(open(\"../data/relevance_dataset/dict_data_20240424.p\", \"rb\"))\n",
    "dict_id_cat = pickle.load(open(\"../data/relevance_dataset/dict_id_cat_20240424.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id2info = dict_data_all['dict_id2info']\n",
    "\n",
    "np.random.seed(2024)\n",
    "num_train_samples = 1000\n",
    "idx_train = np.random.choice(len(dict_id_cat['table']['train']), num_train_samples, replace=False)\n",
    "\n",
    "x_train_table = np.array([dict_id2info[index]['embedding_concat'] for index in dict_id_cat['table']['train']])[idx_train,:]\n",
    "y_train_table = np.array([dict_id2info[index]['label_truth_bool'] for index in dict_id_cat['table']['train']])[idx_train]\n",
    "x_test_table = np.array([dict_id2info[index]['embedding_concat'] for index in dict_id_cat['table']['test']])\n",
    "y_test_table = np.array([dict_id2info[index]['label_truth_bool'] for index in dict_id_cat['table']['test']])\n",
    "\n",
    "# x_train_bed = np.array([dict_id2info[index]['embedding_concat'] for index in dict_id_cat['bed']['train']])\n",
    "# y_train_bed = np.array([dict_id2info[index]['label_truth_bool'] for index in dict_id_cat['bed']['train']])\n",
    "x_test_bed = np.array([dict_id2info[index]['embedding_concat'] for index in dict_id_cat['bed']['test']])\n",
    "y_test_bed = np.array([dict_id2info[index]['label_truth_bool'] for index in dict_id_cat['bed']['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_table = LogisticRegression().fit(x_train_table, y_train_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Table model for table train/test acc: (0.888, 0.8873091100579252)\n",
      " Table model for bed test acc: 0.7199256850905713\n",
      " GPT for bed test acc: 0.7570831398049234\n"
     ]
    }
   ],
   "source": [
    "# direct transfer the table model to bed dataset\n",
    "acc_table_train = np.mean(model_table.predict(x_train_table)==y_train_table)\n",
    "acc_table_test = np.mean(model_table.predict(x_test_table)==y_test_table)\n",
    "acc_table_model_on_bed = np.mean(model_table.predict(x_test_bed)==y_test_bed)\n",
    "acc_gpt_instruct_bed = np.mean(list_pred_gpt_instruct_bed_bool==y_test_bed)\n",
    "print(f' Table model for table train/test acc: {acc_table_train,acc_table_test}\\n Table model for bed test acc: {acc_table_model_on_bed}\\n GPT for bed test acc: {acc_gpt_instruct_bed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)\n",
    "num_transfer_samples = 500\n",
    "idx_transfer = np.random.choice(len(x_test_bed), num_transfer_samples, replace=False)\n",
    "x_train_bed_transfer, y_train_bed_transfer = x_test_bed[idx_transfer], y_test_bed[idx_transfer]\n",
    "x_test_bed_transfer, y_test_bed_transfer = np.delete(x_test_bed, idx_transfer, axis=0), np.delete(y_test_bed, idx_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transfer model for table train/test acc: (0.892, 0.889942074776198)\n",
      " Transfer model for bed train/test acc: (0.76, 0.7652752571082879)\n"
     ]
    }
   ],
   "source": [
    "# train a transfer learning model with the labeled table data and gpt-labeled bed data, as in section 5\n",
    "x_train_transfer = np.concatenate((x_train_table, x_train_bed_transfer), axis=0)\n",
    "y_train_transfer = np.concatenate((y_train_table, np.array(list_pred_gpt_instruct_bed_bool)[idx_transfer]), axis=0)\n",
    "model_transfer = LogisticRegression().fit(x_train_transfer, y_train_transfer)\n",
    "\n",
    "acc_transfer_table_train = np.mean(model_transfer.predict(x_train_table)==y_train_table)\n",
    "acc_transfer_table_test = np.mean(model_transfer.predict(x_test_table)==y_test_table)\n",
    "acc_transfer_bed_train = np.mean(model_transfer.predict(x_train_bed_transfer)==y_train_bed_transfer)\n",
    "acc_transfer_bed_test = np.mean(model_transfer.predict(x_test_bed_transfer)==y_test_bed_transfer)\n",
    "print(f' Transfer model for table train/test acc: {acc_transfer_table_train,acc_transfer_table_test}\\n Transfer model for bed train/test acc: {acc_transfer_bed_train,acc_transfer_bed_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
